[environment]
seed = 42
# Directory to store and consume test cases
test_cases_path = "./gas/data/test_cases"

[model]
# Can be a huggingface model name or full path for a checkpoint file
# e.g.
# mistralai/Ministral-8B-Instruct-2410
# daven3/k2 | tokenizer = huggyllama/llama-7b
# meta-llama/Llama-3.2-3B-Instruct
# meta-llama/Meta-Llama-3.1-8B-Instruct
# google/gemma-2-9b-it
pretrained_model_name_or_path = "/mnt/D-SSD/parraga/LLM-FineTuning/final_models/gemma_eos_fixed-gemma-2-9b-it-Jan14_09-39-19"
# Can be a huggingface model name or full path for a checkpoint file
tokenizer_name_or_path = "/mnt/D-SSD/parraga/LLM-FineTuning/final_models/gemma_eos_fixed-gemma-2-9b-it-Jan14_09-39-19"
# Cache directory to store and consume models.
cache_dir = "/mnt/D-SSD/parraga/LLMs/cache"
should_apply_chat_template = true
peft = true

[generation]
max_new_tokens = 150        # Adjust for shorter output
min_new_tokens = 2          # Adjust for shorter output
# no_repeat_ngrams_size = 2 # Prevent repetitive phrases
# REPETITION_PENALTY = 1.1    # Penalize repetition in word level
penalty_alpha = 1.1         # penalize repetition with context awareness
temperature = 0.2           # Control randomness
top_k = 40                  # Top-k sampling
top_p = 0.75                # Nucleus sampling
do_sample = true            # Enable sampling


[evaluation]
model_judge = "gpt-4o-mini"
results_path = "./results"
